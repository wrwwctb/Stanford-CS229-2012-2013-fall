% Parts adapted from www-anw.cs.umass.edu/rlr/domains.html
% pole-balancing simulation

% The main simulation loop calls
% cart_pole.m for simulating the pole dynamics
% get_state.m for discretizing the state space
% show_cart.m for display. 

% NUM_STATES: Number of states in the discretized state space
% The state numbered NUM_STATES is a special state that marks
% when the pole has fallen or when the cart is out of bounds
% DO NOT treat this state any differently in your code. Any distinctions
% should come automatically from your learning algorithm.

% After each simulation cycle, update the transition
% counts and rewards observed. Do not change either the value function
% or the transition probability matrix at each cycle. 

% Whenever the pole falls, a section below will be
% executed. At this point, use the transition counts and reward
% observations gathered to generate a new model for the MDP
% (i.e., transition probabilities and state rewards). After that, use value
% iteration to get the optimal value function for this MDP model.

% TOLERANCE: Controls the convergence criteria for each value iteration run
% In the value iteration, assume convergence when the maximum
% absolute change in the value function at any state in an iteration
% becomes lower than TOLERANCE.

% Write code that chooses the best action according to the current value
% function, and the current model of the MDP. The action are either 1 or 2
% (corresponding to possible directions of pushing the cart).

% Finally, assume that the simulation has converged when
% 'NO_LEARNING_THRESHOLD' consecutive value function computations all 
% converged within one value function iteration. Intuitively, it seems
% like there will be little learning after this, so we end the simulation
% here, and say the overall algorithm has converged. 

% Learning curves can be generated by calling plot_learning_curve.m (it
% assumes that the learning was just executed, and the array
% time_steps_to_failure that records the time for which the pole was
% balanced before each failure are in memory). num_failures is a variable
% that stores the number of failures (pole drops / cart out of bounds)
% till now.

% simulation display control:

% pause_time: Controls the pause between successive frames of the
% display. Higher values make your simulation slower.
% min_trial_length_to_start_display: Allows you to start the display only
% after the pole has been successfully balanced for at least this many
% trials. Setting this to zero starts the display immediately. Choosing a
% reasonably high value (around 100) can allow you to rush through the
% initial learning quickly, and start the display only after the
% performance is reasonable.

clear all; close all; clc;
pause_time                        = 0.001;
min_trial_length_to_start_display = 10000;
display_started                   = 0;
display_at_all                    = 1;
NUM_STATES                        = 163;
GAMMA                             = 0.995; % Discount factor
TOLERANCE                         = 0.01;
NO_LEARNING_THRESHOLD             = 20;
time                              = 0; % Time step of the simulation
num_failures                      = 0;
time_at_start_of_current_trial    = 0;
max_failures                      = 500; % should be enough for convergence
time_steps_to_failure             = zeros( max_failures, 1 );
x                                 = 0.0; % the actual state vector
x_dot                             = 0.0; % starting state (0 0 0 0)
theta                             = 0.0;
theta_dot                         = 0.0;
state                             = get_state(x, x_dot, theta, theta_dot);
% the number given to this state
% only consider this representation of the state

if display_at_all == 1 &&...
        ( min_trial_length_to_start_display == 0 || display_started == 1 )
    show_cart( x, x_dot, theta, theta_dot, pause_time );
end

seednum   = 9;
stream    = RandStream.create('mt19937ar','seed',seednum);
RandStream.setDefaultStream( stream );

%%% initialization. Assume no transitions or rewards have been observed
value_fun = rand ( [ NUM_STATES 1] ) *.1; % rand (0, 0.10)
trans_pro = ones ( [ NUM_STATES 2 NUM_STATES ]) / NUM_STATES; % uniform
pro_numer = zeros( [ NUM_STATES 2 NUM_STATES ]);
pro_denom = zeros( [ NUM_STATES 2 ]);
rewardfun = zeros( [ NUM_STATES 1 ]);
rewardnum = zeros( [ NUM_STATES 1 ]);
rewardden = zeros( [ NUM_STATES 1 ]);

n_consecutive_good_trials = 0;

while num_failures < max_failures &&...
        n_consecutive_good_trials < NO_LEARNING_THRESHOLD

    action               = fun_choose_action( state, theta, ...
                                              trans_pro, value_fun );
    [ x, x_dot,... % Get the next state by simulating the dynamics
        theta,theta_dot] = cart_pole(action, x, x_dot, theta, theta_dot);
    new_state            = get_state(x, x_dot, theta, theta_dot);
    time                 = time + 1;

    if display_at_all == 1 && display_started == 1
        show_cart( x, x_dot, theta, theta_dot, pause_time );
    end
  
    if new_state == NUM_STATES % Reward function - do not change
        R = -1;
    else
        R = 0; % -abs(theta)/2.0;
    end
    pro_numer( state, action, new_state ) = pro_numer(state,action,...
                                                      new_state) + 1;
    pro_denom( state, action )            = pro_denom(state,action) + 1;
    rewardnum( new_state )                = rewardnum( new_state ) + R;
    rewardden( new_state )                = rewardden( new_state ) + 1;
    % in order to treat the fail state as an ordinary state, we observe the
    % reward function that is deterministic. ridiculous tho
    if new_state == NUM_STATES % Recompute MDP model whenever pole falls
        trans_pro = fun_division_1( pro_numer, pro_denom, NUM_STATES );
        rewardfun = fun_division_2( rewardnum, rewardden );
        [ value_fun, n_iter ] = fun_value_iter_synchronous( TOLERANCE,...
            GAMMA, NUM_STATES, rewardfun, trans_pro, value_fun );
        if n_iter == 1
            n_consecutive_good_trials = n_consecutive_good_trials + 1;
        else
            n_consecutive_good_trials = 0;
        end
    end
    
    if new_state == NUM_STATES % simulation control and reinitialization
        num_failures                          = num_failures + 1;
        time_steps_to_failure( num_failures ) = time -...
                                            time_at_start_of_current_trial;
        time_at_start_of_current_trial        = time;
        if display_at_all == 1
            disp( time_steps_to_failure( num_failures ) );
            if time_steps_to_failure( num_failures ) >...
                    min_trial_length_to_start_display
                display_started = 1;
            end
        end
        x         = -1.1 + rand(1)*2.2; % 0.0;
        x_dot     = 0.0;
        theta     = 0.0;
        theta_dot = 0.0;
        state     = get_state(x, x_dot, theta, theta_dot);
    else 
        state = new_state;
    end
end
disp( [ 'num_failures/max_failures:'...
         num2str(num_failures) '/' num2str(max_failures) sprintf('\n')...
        'max( time_steps_to_failure ):'...
         num2str( max( time_steps_to_failure )) ] );
plot_learning_curve % ( time balanced vs trial )
figure;
colormap('gray');
subplot(1,2,1);
imagesc(log10(squeeze(trans_pro(:,1,:))));
subplot(1,2,2);
imagesc(log10(squeeze(trans_pro(:,2,:))));